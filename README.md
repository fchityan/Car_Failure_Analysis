ğŸ“Œ Project Overview

This project presents an end-to-end machine learning workflow for analyzing car failure data, from exploratory data analysis (EDA) through feature engineering, model training, tuning, and evaluation.

The notebook is designed to be:

	â€¢	ğŸ““ Self-contained and reproducible
	â€¢	ğŸ”¬ Exploratory yet production-aware
	â€¢	ğŸ¤– Suitable for benchmarking multiple model types

All analysis, preprocessing, and modeling steps are executed directly within the notebook.

ğŸ¯ Objective

The goal is to build a reliable classification pipeline that:

	â€¢	Identifies patterns associated with vehicle failures
	â€¢	Handles data quality issues and feature imbalance
	â€¢	Evaluates the trade-offs between accuracy and class balance
	â€¢	Compares multiple machine learning models using consistent preprocessing

 ğŸ“Š Key Findings from Exploratory Data Analysis

Initial EDA revealed several important characteristics of the dataset:

	â€¢	âš ï¸ Negative values present in RPM
	â€¢	ğŸ“ˆ Fuel consumption is slightly right-skewed
	â€¢	ğŸŒ¡ï¸ Temperature forms two clearly separable clusters
	â€¢	ğŸš— High cardinality in car model, making one-hot encoding impractical
	â€¢	ğŸŒ³ Categorical feature distributions vary widely, favoring tree-based models
	â€¢	ğŸ”§ Each car can experience only one failure type
	â€¢	âš–ï¸ Strong target imbalance (>80% non-failure cases)

Due to the imbalance, down-sampling improves class balance but may reduce overall accuracy.

ğŸ” Pipeline Stages

The notebook follows a structured 6-step pipeline:

	1.	ğŸ§¹ Data Cleaning
	2.	ğŸ·ï¸ Feature Encoding
	3.	ğŸ§ª Data Preparation
	4.	ğŸ¤– Model Training
	5.	ğŸ¯ Hyperparameter Tuning
	6.	ğŸ“ˆ Model Evaluation

 ğŸ“¦ Initial Dataset Size
 
	â€¢	10,081 rows
	â€¢	14 columns

 ğŸŒ¡ï¸ Temperature Encoding Rationale

The distribution of temperature values showed a clean separation into two low-variance clusters. Encoding these as High and Low reduces noise while preserving signal.

After cleaning:

	â€¢	9,857 rows
	â€¢	13 columns

 ğŸ·ï¸ Feature Encoding

After cleaning:

	â€¢	Numerical features: RPM, Fuel Consumption
	â€¢	All other features are categorical

Encoding Strategy
	â€¢	ğŸ”¢ Mean weight encoding for high-cardinality features
	â€¢	ğŸ”  One-hot encoding for low-cardinality features
	â€¢	ğŸ”§ Failure columns merged into a single Faults column

After encoding:

	â€¢	9,857 rows
	â€¢	23 columns

ğŸ§ª Data Preparation

The notebook supports optional preparation steps:

	â€¢	ğŸ“‰ Log transformation of fuel consumption
	â€¢	âš–ï¸ Target label balancing
	â€¢	ğŸ“ Feature standardization

After preparation, the dataset is split into training and testing sets (70% / 30%).

ğŸ“‰ Dataset Size After Preparation

Before Train/Test Split:

	â€¢	Balanced dataset: 1,496 rows
	â€¢	Unbalanced dataset: 9,857 rows

 ğŸ§  Modeling Approach

The notebook evaluates:

	â€¢	Baseline classifiers for performance benchmarking
	â€¢	Multiple model families to assess biasâ€“variance trade-offs
	â€¢	Hyperparameter tuning using cross-validation
	â€¢	Performance metrics appropriate for imbalanced data

 âœ… Key Takeaways
 
	â€¢	ğŸ§¹ Strong preprocessing improves model stability
	â€¢	âš–ï¸ Balancing improves minority-class detection at an accuracy cost
	â€¢	ğŸŒ³ Tree-based models perform well with mixed feature types
	â€¢	ğŸ“Š Proper encoding dramatically reduces dimensionality
